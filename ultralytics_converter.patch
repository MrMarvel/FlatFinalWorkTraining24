diff --git a/nn/modules/block.py b/nn/modules/block.py
index d8183d8..c6e47e1 100644
--- a/nn/modules/block.py
+++ b/nn/modules/block.py
@@ -176,17 +176,24 @@ class C2f(nn.Module):
         self.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n))
 
     def forward(self, x):
-        """Forward pass through C2f layer."""
-        y = list(self.cv1(x).chunk(2, 1))
-        y.extend(m(y[-1]) for m in self.m)
-        return self.cv2(torch.cat(y, 1))
+        # y = list(self.cv1(x).split((self.c, self.c), 1))
+        # y.extend(m(y[-1]) for m in self.m)
+        # return self.cv2(torch.cat(y, 1))
 
+        print("ook")
+        x = self.cv1(x)
+        x = [x, x[:, self.c:, ...]]
+        x.extend(m(x[-1]) for m in self.m)
+        x.pop(1)
+        return self.cv2(torch.cat(x, 1))
+    
     def forward_split(self, x):
-        """Forward pass using split() instead of chunk()."""
-        y = list(self.cv1(x).split((self.c, self.c), 1))
-        y.extend(m(y[-1]) for m in self.m)
-        return self.cv2(torch.cat(y, 1))
-
+        print("ook")
+        x = self.cv1(x)
+        x = [x, x[:, self.c:, ...]]
+        x.extend(m(x[-1]) for m in self.m)
+        x.pop(1)
+        return self.cv2(torch.cat(x, 1))
 
 class C3(nn.Module):
     """CSP Bottleneck with 3 convolutions."""
diff --git a/nn/modules/head.py b/nn/modules/head.py
index 0b02eb3..d39027c 100644
--- a/nn/modules/head.py
+++ b/nn/modules/head.py
@@ -51,25 +51,27 @@ class Detect(nn.Module):
             self.anchors, self.strides = (x.transpose(0, 1) for x in make_anchors(x, self.stride, 0.5))
             self.shape = shape
 
-        x_cat = torch.cat([xi.view(shape[0], self.no, -1) for xi in x], 2)
-        if self.export and self.format in ('saved_model', 'pb', 'tflite', 'edgetpu', 'tfjs'):  # avoid TF FlexSplitV ops
-            box = x_cat[:, :self.reg_max * 4]
-            cls = x_cat[:, self.reg_max * 4:]
-        else:
-            box, cls = x_cat.split((self.reg_max * 4, self.nc), 1)
-        dbox = dist2bbox(self.dfl(box), self.anchors.unsqueeze(0), xywh=True, dim=1) * self.strides
-
-        if self.export and self.format in ('tflite', 'edgetpu'):
-            # Normalize xywh with image size to mitigate quantization error of TFLite integer models as done in YOLOv5:
-            # https://github.com/ultralytics/yolov5/blob/0c8de3fca4a702f8ff5c435e67f378d1fce70243/models/tf.py#L307-L309
-            # See this PR for details: https://github.com/ultralytics/ultralytics/pull/1695
-            img_h = shape[2] * self.stride[0]
-            img_w = shape[3] * self.stride[0]
-            img_size = torch.tensor([img_w, img_h, img_w, img_h], device=dbox.device).reshape(1, 4, 1)
-            dbox /= img_size
-
-        y = torch.cat((dbox, cls.sigmoid()), 1)
-        return y if self.export else (y, x)
+        # x_cat = torch.cat([xi.view(shape[0], self.no, -1) for xi in x], 2)
+        # if self.export and self.format in ('saved_model', 'pb', 'tflite', 'edgetpu', 'tfjs'):  # avoid TF FlexSplitV ops
+        #     box = x_cat[:, :self.reg_max * 4]
+        #     cls = x_cat[:, self.reg_max * 4:]
+        # else:
+        #     box, cls = x_cat.split((self.reg_max * 4, self.nc), 1)
+        # dbox = dist2bbox(self.dfl(box), self.anchors.unsqueeze(0), xywh=True, dim=1) * self.strides
+
+        # if self.export and self.format in ('tflite', 'edgetpu'):
+        #     # Normalize xywh with image size to mitigate quantization error of TFLite integer models as done in YOLOv5:
+        #     # https://github.com/ultralytics/yolov5/blob/0c8de3fca4a702f8ff5c435e67f378d1fce70243/models/tf.py#L307-L309
+        #     # See this PR for details: https://github.com/ultralytics/ultralytics/pull/1695
+        #     img_h = shape[2] * self.stride[0]
+        #     img_w = shape[3] * self.stride[0]
+        #     img_size = torch.tensor([img_w, img_h, img_w, img_h], device=dbox.device).reshape(1, 4, 1)
+        #     dbox /= img_size
+
+        # y = torch.cat((dbox, cls.sigmoid()), 1)
+        # return y if self.export else (y, x)
+        print("ook")
+        return torch.cat([xi.view(shape[0], self.no, -1) for xi in x], 2).permute(0, 2, 1)
 
     def bias_init(self):
         """Initialize Detect() biases, WARNING: requires stride availability."""
